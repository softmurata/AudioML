{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/AudioML/blob/main/avatar/sadtalkerjobforsvc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0u1Nwo8v4MP"
      },
      "source": [
        "Base Code\n",
        "\n",
        "[colab](https://colab.research.google.com/github/Winfredy/SadTalker/blob/main/quick_demo.ipynb#scrollTo=fAjwGmKKYl_I)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFGEI01SvWqg"
      },
      "source": [
        "Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFFMBjTYuR0M",
        "outputId": "e6feeba3-ba3c-47db-d746-0ee214aab054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla T4, 15360 MiB, 15101 MiB\n"
          ]
        }
      ],
      "source": [
        "### make sure that CUDA is available in Edit -> Nootbook settings -> GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE7GmtI9vbxO"
      },
      "outputs": [],
      "source": [
        "!update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.8 2\n",
        "!update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.9 1\n",
        "!sudo apt install python3.8\n",
        "\n",
        "!sudo apt-get install python3.8-distutils\n",
        "\n",
        "!python --version\n",
        "\n",
        "!apt-get update\n",
        "\n",
        "!apt install software-properties-common\n",
        "\n",
        "!sudo dpkg --remove --force-remove-reinstreq python3-pip python3-setuptools python3-wheel\n",
        "\n",
        "!apt-get install python3-pip\n",
        "\n",
        "print('Git clone project and install requirements...')\n",
        "!git clone https://github.com/Winfredy/SadTalker &> /dev/null\n",
        "%cd SadTalker\n",
        "!export PYTHONPATH=/content/SadTalker:$PYTHONPATH\n",
        "!python3.8 -m pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!apt update\n",
        "!apt install ffmpeg &> /dev/null\n",
        "!python3.8 -m pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvjlPb93vhsd"
      },
      "source": [
        "Download weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymvda_PmvjZ0"
      },
      "outputs": [],
      "source": [
        "# download scriptsを変更する -O -> -Pに\n",
        "print('Download pre-trained models...')\n",
        "!rm -rf checkpoints\n",
        "!bash scripts/download_models.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ESEO7nivnQ8"
      },
      "outputs": [],
      "source": [
        "# borrow from makeittalk\n",
        "import ipywidgets as widgets\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"Choose the image name to animate: (saved in folder 'examples/')\")\n",
        "img_list = glob.glob1('examples/source_image', '*.png')\n",
        "img_list.sort()\n",
        "img_list = [item.split('.')[0] for item in img_list]\n",
        "default_head_name = widgets.Dropdown(options=img_list, value='full3')\n",
        "def on_change(change):\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\n",
        "        plt.imshow(plt.imread('examples/source_image/{}.png'.format(default_head_name.value)))\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "default_head_name.observe(on_change)\n",
        "display(default_head_name)\n",
        "plt.imshow(plt.imread('examples/source_image/{}.png'.format(default_head_name.value)))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cig1Lj0Ovn2L"
      },
      "source": [
        "inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Iyqn_88DGLD"
      },
      "outputs": [],
      "source": [
        "# inference.py\n",
        "\"\"\"\n",
        "from glob import glob\n",
        "import shutil\n",
        "import torch\n",
        "from time import  strftime\n",
        "import os, sys, time\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from src.utils.preprocess import CropAndExtract\n",
        "from src.test_audio2coeff import Audio2Coeff\n",
        "from src.facerender.animate import AnimateFromCoeff\n",
        "from src.generate_batch import get_data\n",
        "from src.generate_facerender_batch import get_facerender_data\n",
        "from src.utils.init_path import init_path\n",
        "\n",
        "def main(args):\n",
        "    #torch.backends.cudnn.enabled = False\n",
        "\n",
        "    pic_path = args.source_image\n",
        "    audio_path = args.driven_audio\n",
        "    save_dir = os.path.join(args.result_dir, strftime(\"%Y_%m_%d_%H.%M.%S\"))\n",
        "    save_path = args.save_path\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    pose_style = args.pose_style\n",
        "    device = args.device\n",
        "    batch_size = args.batch_size\n",
        "    input_yaw_list = args.input_yaw\n",
        "    input_pitch_list = args.input_pitch\n",
        "    input_roll_list = args.input_roll\n",
        "    ref_eyeblink = args.ref_eyeblink\n",
        "    ref_pose = args.ref_pose\n",
        "\n",
        "    current_root_path = os.path.split(sys.argv[0])[0]\n",
        "\n",
        "    sadtalker_paths = init_path(args.checkpoint_dir, os.path.join(current_root_path, 'src/config'), args.size, args.old_version, args.preprocess)\n",
        "\n",
        "    #init model\n",
        "    preprocess_model = CropAndExtract(sadtalker_paths, device)\n",
        "\n",
        "    audio_to_coeff = Audio2Coeff(sadtalker_paths,  device)\n",
        "\n",
        "    animate_from_coeff = AnimateFromCoeff(sadtalker_paths, device)\n",
        "\n",
        "    #crop image and extract 3dmm from image\n",
        "    first_frame_dir = os.path.join(save_dir, 'first_frame_dir')\n",
        "    os.makedirs(first_frame_dir, exist_ok=True)\n",
        "    print('3DMM Extraction for source image')\n",
        "    first_coeff_path, crop_pic_path, crop_info =  preprocess_model.generate(pic_path, first_frame_dir, args.preprocess,\\\n",
        "                                                                             source_image_flag=True, pic_size=args.size)\n",
        "    if first_coeff_path is None:\n",
        "        print(\"Can't get the coeffs of the input\")\n",
        "        return\n",
        "\n",
        "    if ref_eyeblink is not None:\n",
        "        ref_eyeblink_videoname = os.path.splitext(os.path.split(ref_eyeblink)[-1])[0]\n",
        "        ref_eyeblink_frame_dir = os.path.join(save_dir, ref_eyeblink_videoname)\n",
        "        os.makedirs(ref_eyeblink_frame_dir, exist_ok=True)\n",
        "        print('3DMM Extraction for the reference video providing eye blinking')\n",
        "        ref_eyeblink_coeff_path, _, _ =  preprocess_model.generate(ref_eyeblink, ref_eyeblink_frame_dir, args.preprocess, source_image_flag=False)\n",
        "    else:\n",
        "        ref_eyeblink_coeff_path=None\n",
        "\n",
        "    if ref_pose is not None:\n",
        "        if ref_pose == ref_eyeblink:\n",
        "            ref_pose_coeff_path = ref_eyeblink_coeff_path\n",
        "        else:\n",
        "            ref_pose_videoname = os.path.splitext(os.path.split(ref_pose)[-1])[0]\n",
        "            ref_pose_frame_dir = os.path.join(save_dir, ref_pose_videoname)\n",
        "            os.makedirs(ref_pose_frame_dir, exist_ok=True)\n",
        "            print('3DMM Extraction for the reference video providing pose')\n",
        "            ref_pose_coeff_path, _, _ =  preprocess_model.generate(ref_pose, ref_pose_frame_dir, args.preprocess, source_image_flag=False)\n",
        "    else:\n",
        "        ref_pose_coeff_path=None\n",
        "\n",
        "    #audio2ceoff\n",
        "    batch = get_data(first_coeff_path, audio_path, device, ref_eyeblink_coeff_path, still=args.still)\n",
        "    coeff_path = audio_to_coeff.generate(batch, save_dir, pose_style, ref_pose_coeff_path)\n",
        "\n",
        "    # 3dface render\n",
        "    if args.face3dvis:\n",
        "        from src.face3d.visualize import gen_composed_video\n",
        "        gen_composed_video(args, device, first_coeff_path, coeff_path, audio_path, os.path.join(save_dir, '3dface.mp4'))\n",
        "\n",
        "    #coeff2video\n",
        "    data = get_facerender_data(coeff_path, crop_pic_path, first_coeff_path, audio_path,\n",
        "                                batch_size, input_yaw_list, input_pitch_list, input_roll_list,\n",
        "                                expression_scale=args.expression_scale, still_mode=args.still, preprocess=args.preprocess, size=args.size)\n",
        "\n",
        "    result = animate_from_coeff.generate(data, save_dir, pic_path, crop_info, \\\n",
        "                                enhancer=args.enhancer, background_enhancer=args.background_enhancer, preprocess=args.preprocess, img_size=args.size)\n",
        "\n",
        "\n",
        "    shutil.move(result, save_path)\n",
        "    print('The generated video is named:', save_path)\n",
        "\n",
        "    if not args.verbose:\n",
        "        shutil.rmtree(save_dir)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument(\"--driven_audio\", default='./examples/driven_audio/bus_chinese.wav', help=\"path to driven audio\")\n",
        "    parser.add_argument(\"--source_image\", default='./examples/source_image/full_body_1.png', help=\"path to source image\")\n",
        "    parser.add_argument(\"--ref_eyeblink\", default=None, help=\"path to reference video providing eye blinking\")\n",
        "    parser.add_argument(\"--ref_pose\", default=None, help=\"path to reference video providing pose\")\n",
        "    parser.add_argument(\"--checkpoint_dir\", default='./checkpoints', help=\"path to output\")\n",
        "    parser.add_argument(\"--result_dir\", default='./results', help=\"path to output\")\n",
        "    parser.add_argument(\"--pose_style\", type=int, default=0,  help=\"input pose style from [0, 46)\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=2,  help=\"the batch size of facerender\")\n",
        "    parser.add_argument(\"--size\", type=int, default=256,  help=\"the image size of the facerender\")\n",
        "    parser.add_argument(\"--expression_scale\", type=float, default=1.,  help=\"the batch size of facerender\")\n",
        "    parser.add_argument('--input_yaw', nargs='+', type=int, default=None, help=\"the input yaw degree of the user \")\n",
        "    parser.add_argument('--input_pitch', nargs='+', type=int, default=None, help=\"the input pitch degree of the user\")\n",
        "    parser.add_argument('--input_roll', nargs='+', type=int, default=None, help=\"the input roll degree of the user\")\n",
        "    parser.add_argument('--enhancer',  type=str, default=None, help=\"Face enhancer, [gfpgan, RestoreFormer]\")\n",
        "    parser.add_argument('--background_enhancer',  type=str, default=None, help=\"background enhancer, [realesrgan]\")\n",
        "    parser.add_argument(\"--cpu\", dest=\"cpu\", action=\"store_true\")\n",
        "    parser.add_argument(\"--face3dvis\", action=\"store_true\", help=\"generate 3d face and 3d landmarks\")\n",
        "    parser.add_argument(\"--still\", action=\"store_true\", help=\"can crop back to the original videos for the full body aniamtion\")\n",
        "    parser.add_argument(\"--preprocess\", default='crop', choices=['crop', 'extcrop', 'resize', 'full', 'extfull'], help=\"how to preprocess the images\" )\n",
        "    parser.add_argument(\"--verbose\",action=\"store_true\", help=\"saving the intermedia output or not\" )\n",
        "    parser.add_argument(\"--old_version\",action=\"store_true\", help=\"use the pth other than safetensor version\" )\n",
        "    parser.add_argument(\"--save_path\", type=str, default=\"\")\n",
        "\n",
        "    # net structure and parameters\n",
        "    parser.add_argument('--net_recon', type=str, default='resnet50', choices=['resnet18', 'resnet34', 'resnet50'], help='useless')\n",
        "    parser.add_argument('--init_path', type=str, default=None, help='Useless')\n",
        "    parser.add_argument('--use_last_fc',default=False, help='zero initialize the last fc')\n",
        "    parser.add_argument('--bfm_folder', type=str, default='./checkpoints/BFM_Fitting/')\n",
        "    parser.add_argument('--bfm_model', type=str, default='BFM_model_front.mat', help='bfm model')\n",
        "\n",
        "    # default renderer parameters\n",
        "    parser.add_argument('--focal', type=float, default=1015.)\n",
        "    parser.add_argument('--center', type=float, default=112.)\n",
        "    parser.add_argument('--camera_d', type=float, default=10.)\n",
        "    parser.add_argument('--z_near', type=float, default=5.)\n",
        "    parser.add_argument('--z_far', type=float, default=15.)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if torch.cuda.is_available() and not args.cpu:\n",
        "        args.device = \"cuda\"\n",
        "    else:\n",
        "        args.device = \"cpu\"\n",
        "\n",
        "    main(args)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Bcpl1Lm3kvb"
      },
      "outputs": [],
      "source": [
        "# !wget \"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/ac8c7c20-c499-49de-aa8f-5a2c9644f300/width=512/00001-1113583880-best%20quality,%20highres,%20photo-realistic,%201%20girl,%20solo,(Anya%20Forger_1.5),%20(child_1.2),%20(10%20year%20old%20girl),%20(young%20girl),%20(flat%20che.jpeg\" -O /content/anya.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n37sRudNDQsq",
        "outputId": "3a6191c4-2ec8-4d00-bfbe-1860f005ef68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Archive:  /content/case3.zip\n",
            "   creating: content/case3/\n",
            "  inflating: content/case3/000.wav   \n",
            "  inflating: content/case3/001.wav   \n",
            "  inflating: content/case3/002.wav   \n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!unzip /content/case3.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7k2ETVv4dJB"
      },
      "outputs": [],
      "source": [
        "# query: anya civitai realistic\n",
        "!wget https://pds.exblog.jp/pds/1/202307/03/31/f0407631_05391473.jpeg -O /content/anya.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmY2dZD8vrUf"
      },
      "outputs": [],
      "source": [
        "# selected audio from exmaple/driven_audio\n",
        "%cd /content/SadTalker\n",
        "# img = 'examples/source_image/{}.png'.format(default_head_name.value)\n",
        "img = \"/content/anya.jpg\"\n",
        "import glob\n",
        "import os\n",
        "\n",
        "case_name = \"case3\"\n",
        "os.makedirs(f\"/content/{case_name}\", exist_ok=True)\n",
        "\n",
        "wavfiles = sorted(glob.glob(f\"/content/content/{case_name}/*.wav\"))\n",
        "for wav_file in wavfiles:\n",
        "  wav_id = wav_file.split(\"/\")[-1].split(\".\")[0]\n",
        "  save_path = f\"/content/{case_name}/{wav_id}.mp4\"\n",
        "  !python3.8 inference.py --driven_audio {wav_file} \\\n",
        "           --source_image {img} \\\n",
        "           --result_dir ./results --still --preprocess full --enhancer gfpgan --save_path {save_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwu35dJVv0tl"
      },
      "source": [
        "Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaqL2EAAv0CH"
      },
      "outputs": [],
      "source": [
        "# visualize code from makeittalk\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import os, sys\n",
        "\n",
        "# get the last from results\n",
        "\n",
        "\n",
        "mp4_name = \"/content/case2/000.mp4\"\n",
        "\n",
        "mp4 = open('{}'.format(mp4_name),'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "print('Display animation: {}'.format(mp4_name), file=sys.stderr)\n",
        "display(HTML(\"\"\"\n",
        "  <video width=256 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\n",
        "  \"\"\" % data_url))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqCXDLzvwXeu"
      },
      "outputs": [],
      "source": [
        "# 多分ちゃんと作り込むならchatanythingのsadtalker部分を読み込む必要がありそう\n",
        "# https://github.com/zhoudaquan/ChatAnything/tree/main/chat_anything/sad_talker"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WebUI"
      ],
      "metadata": {
        "id": "igP9Bw7INXJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "ZifVkBXTNX80"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBABHF/UYnMK2RbbezeLC1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}